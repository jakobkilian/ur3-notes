# UR3 Notes and Findings
Controlling a Universal Robots UR3 is far from trivial, especially when it comes to performing smooth, real-time movements based on sensor input. In addition to the inherent complexity of programming crash-free 3D movement UR lacks proper documentation and sometimes provides buggy installers. 
**In this repo I wanted to share notes and findings of my experiments with the UR3** to give other people an easier start.
These notes were made as part of my work at KISD, Cologne, and refer to the UR3 model that was available in my lab there. In principle, they can be applied to other UR models, but would need to be adapted accordingly...

## Universal Interface via OSC/MQTT over Network
UR offers a [bunch of possibilities to interface with the UR3](https://www.universal-robots.com/articles/ur/interface-communication/overview-of-client-interfaces/). I looked through this list, but they all had drawbacks in terms of compatibility and universality. E.g. "RTDE" required installations on the client side â€“ some having issues about the installation which haven't been solved for years at the time. On top connecting to UR controller by default only works with Ethernet and fix IP addresses. Having e.g. microcontrollers ina WiFi network to control the robot would be tricky in this setup. So I wondered why there wasn't a universal interface that could be used to remotely control the robot from any programming language and any device via network. That's why I build a OSC/MQTT bridge to control the robot from within e.g. python, MaxMSP, Processing or others on any device in a network (Ethernet/WiFi) set up by an additional intermediate Raspberry Pi. This worked very well for my use cases and can be rebuild by following the instructions in the repository

## Fluent Real Time Movements
A main goal of my experiments was to achieve fluent movements based on real time input coming from software that is based on sensory input. In contrast to the usual areas of application for robots in industry, where the start and end of movements and their timing are known, the challenge here is that at no point in time is it known where the robot should move to in the next moment or in the next time window. In most industry-standard applications for robots of this type, the start and end points of movements and their timing are known. Depending on the application, commands such as movej or movel are used, which allow the arm to be moved with parameters such as time, acceleration, maximum speeds, and interpolating blends between points. However, the principle is always the same: start at A, accelerate according to the parameters, slow down, and stop at point B.
However, if the robot is supposed to follow a hand movement, for example, or move along vector paths in a smooth, uninterrupted motion, it becomes more difficult: sending many individual points would cause it to stop at each one. The challenge is that at no point is it known where the robot should move to in the next moment or in the next time window.

https://github.com/user-attachments/assets/80e360dc-4d4e-47b8-9b32-1b1017f62e1a

